import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.applications import EfficientNetB3
from tensorflow.keras.optimizers import Adam
from sklearn.utils.class_weight import compute_class_weight
import matplotlib.pyplot as plt
from PIL import Image

# Old Dataset
# data_dir = 'Datasets/Edited'
# New Dataset
data_dir = 'Dataset/Edited_Enhanced'

# Image Parameters
img_height, img_width = 300, 300
batch_size = 32
epochs = 100

# Image Data Preparations and Adaption 
datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2,
    horizontal_flip=True,
    vertical_flip=True,
    rotation_range=45,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2
)

# Loading Training and Validation Data
train_generator = datagen.flow_from_directory(
    data_dir,
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='categorical',
    subset='training'
)

validation_generator = datagen.flow_from_directory(
    data_dir,
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='categorical',
    subset='validation'
)

# Pre-trained EfficientNetB3 Model
base_model = EfficientNetB3(input_shape=(img_height, img_width, 3), include_top=False, weights='imagenet')
base_model.trainable = False

model = Sequential([
    base_model,
    Flatten(),
    Dense(512, activation='relu'),
    Dropout(0.5),
    Dense(train_generator.num_classes, activation='softmax')
])

# Model Compilation
model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])

# Callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
model_checkpoint = ModelCheckpoint('efficientnetb3_checkpoint.keras', save_best_only=True, monitor='val_loss')

# Model Training
train_ds = tf.data.Dataset.from_generator(
    lambda: train_generator,
    output_types=(tf.float32, tf.float32),
    output_shapes=([None, img_height, img_width, 3], [None, train_generator.num_classes])
).repeat()

val_ds = tf.data.Dataset.from_generator(
    lambda: validation_generator,
    output_types=(tf.float32, tf.float32),
    output_shapes=([None, img_height, img_width, 3], [None, validation_generator.num_classes])
).repeat()

history = model.fit(
    train_ds,
    steps_per_epoch=train_generator.samples // batch_size,
    validation_data=val_ds,
    validation_steps=validation_generator.samples // batch_size,
    epochs=epochs,
    callbacks=[early_stopping, model_checkpoint],
    verbose=1
)

# Model Evaluation
loss, accuracy = model.evaluate(validation_generator)
print(f'Validation accuracy: {accuracy * 100:.2f}%')

# Model Saving
model.save('cnn_efficientnetb3.keras')

# Current Accuracy
print(f'Current accuracy: {accuracy * 100:.2f}%')


# EfficientNetB3:
# 1. Architecture: EfficientNetB3 is a larger variant of EfficientNet, offering increased model depth, width, and input resolution compared to EfficientNetB0.
# 2. Parameters: It has 12 million parameters, providing more capacity for learning complex features and achieving higher accuracy.
# 3. Performance: Offers superior accuracy on image classification tasks compared to EfficientNetB0, suitable for applications demanding higher precision.
# 4. Scalability: Maintains efficiency principles of the EfficientNet family but scales up to handle more demanding tasks with greater computational resources.
# 5. Use Case: Useful for tasks where achieving the highest possible accuracy is critical and where the computational infrastructure can support its larger model size.
